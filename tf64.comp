#version 460
// We aren't using any of these, but just in case we want to later...
//#extension GL_ARB_separate_shader_objects : enable
//#extension GL_ARB_gpu_shader_int64 : enable
//#extension GL_EXT_debug_printf : enable
//#extension GL_KHR_shader_subgroup_vote : enable
//#extension GL_EXT_shader_explicit_arithmetic_types_int8 : enable
//#extension GL_EXT_shader_explicit_arithmetic_types_int16 : enable
#extension GL_EXT_shader_explicit_arithmetic_types_int64 : enable
//#extension GL_EXT_shader_atomic_int64: enable

// This SPIR-V from this file is compiled into the gvtf binary, selected with version=192 or version=256
//
// This is version using 64-bit based 128/192/256-bit math. Seems to be faster on nvidia than the 32-bit version.
//
// compile this file into SPIR-V with:
//   glslangValidator --target-env vulkan1.3 -V tf64.comp -o comp.spv
// or
//   glslc --target-env=vulkan1.3 tf64.comp -o comp.spv
// glslc might produce slightly faster SPIR-V for the Radeon VII
//
//
// Installing the 'vulkan-amdgpu' package will result in a significant (2.5x) performance increase.
// also, maybe the difference isn't so much when not using uint64_t math.
//

// Test for (3,5)mod8, and (0)mod(primes 3 -> 23) in one shot. From 446,185,740 potential K-values,
// a list of 72,990,720 are left to TF test on the GPU. List is an array of 32-bit uints, using
// about 278MB.  Each thread takes an offset from the list, adds it to the 96-bit base-K, then
// computes P * K * 2 + 1, which is then TF tested.
// When the entire list has been tested, K-base += M.
#define M (4 * 3 * 5 * 7 * 11 * 13 * 17 * 19 * 23)  // 446,185,740

#define MnLen 22
const uint Mn[MnLen] = {
	(179 * 181 * 191),                  // 0 -   6,188,209
	(193 * 197 * 199),                  // 1 -   7,566,179
	(29 * 31 * 37 * 41 * 43),           // 2 -  58,642,669
	(47 * 53 * 59 * 61),                // 3 -   8,965,109
	(67 * 71 * 73 * 79),                // 4 -  72,370,439
	(83 * 89 * 97 * 101),               // 5 -  27,433,619
	(103 * 107 * 109 * 113),            // 6 - 135,745,657
	(127 * 131 * 137),                  // 7 -   2,279,269
	(149 * 151 * 157),                  // 8 -   3,532,343
	(163 * 167 * 173),                  // 9 -   4,709,233
	(211 * 223 * 227),                  // 10-  10,775,137
	(229 * 233 * 239),                  // 11-  12,752,323
	(241 * 251 * 257),                  // 12-  15,546,187
	(139 * 263 * 269),                  // 13-   9,833,833
	(271 * 277 * 281),                  // 14-  21,093,827
	(283 * 293 * 307),                  // 15-  25,456,133
	(311 * 313 * 317),                  // 16-  30,857,731
	(331 * 337 * 347),                  // 17-  38,706,809
	(349 * 353 * 359),                  // 18-  44,227,723
	(367 * 373 * 379),                  // 19-  51,881,689
	(383 * 389 * 397),                  // 20-  59,147,839
	(401 * 409 * 419),                  // 21-  68,719,771
//	(421 * 431 * 433),                  // 22-  78,568,283
};
#define ListN 72990720
layout (local_size_x = 64) in;

// This is allocated in HOST_VISIBLE_LOCAL memory, and is shared with host.
// it is somewhat slow, compared to DEVICE_LOCAL memory.
layout(binding = 0) buffer buf
{	
	uint64_t    P;          // input from CPU side
	uint        Init;       // controls the code path in main()
	uint        Big;        // Need > 96-bit math
	uint64_t    K[2];       // base K input from CPU side
	uint64_t    Found[10][2];   // output to tell the CPU we found a K resulting in a factor
	uint        NFound;
	uint        Debug[4];   // output only used for debugging
	uint        L3;
	uint        Test[1000];
};

struct uint128 {
	uint64_t Hi, Lo;
};

// This is allocated in DEVICE_LOCAL memory, not shared with host.  See CPU code to see how this is allocated.
// This is much faster to access from the shader, especially if the GPU is in a PCIx1 slot.
layout(binding = 1) buffer buf2
{
	uint    xL, xL2, xLl;
	uint       List[ListN];
	uint       List2[ListN];
	int        PreTop;
	uint128    PreSq;
	uint    Xx[MnLen][1+Mn[6]/32];
};


struct uint256 {
	uint128 H, L;
};
struct uint192 {
	uint64_t H;
	uint128 L;
};

void Set(inout uint256 A, uint128 B) {
	A.L = B;
	A.H.Lo = 0;
	A.H.Hi = 0;
}
void Lsh(inout uint128 A) {
	uint64_t c = A.Lo & (uint64_t(1) << 63);
	A.Lo <<= 1;
	A.Hi <<= 1;
	A.Hi |= c >> 63;
}
void Lsh(inout uint256 A) {
	
	uint64_t c = A.L.Hi & (uint64_t(1) << 63);
	Lsh(A.L);
	Lsh(A.H);
	A.H.Lo |= c >> 63;
}
void Lsh(inout uint192 A) {
	
	uint64_t c = A.L.Hi & (uint64_t(1) << 63);
	Lsh(A.L);
	A.H <<= 1;
	A.H |= c >> 63;
}

void Rsh(inout uint128 A) {
	uint64_t b = A.Hi & 1l;
	A.Hi >>= 1;
	A.Lo >>= 1;
	A.Lo |= b << 63;
}
void Rsh(inout uint192 A) {
	Rsh(A.L);
	uint64_t b = A.H & 1l;
	A.H >>= 1;
	A.L.Hi |= b << 63;
	
}
void Rsh(inout uint256 A) {
	Rsh(A.L);
	uint64_t b = A.H.Lo & 1l;
	Rsh(A.H);
	A.L.Hi |= b << 63;
	
}
int Cmp(uint128 A, uint64_t B) {
	if (A.Hi != 0) {return 1;}
	if (A.Lo > B) {return 1;}
	if (A.Lo < B) {return -1;}
	return 0;
	
}
int Cmp(uint128 A, uint128 B) {
	if (A.Hi > B.Hi || (A.Hi == B.Hi && A.Lo > B.Lo)) {return 1;}
	if (A.Hi < B.Hi || (A.Hi == B.Hi && A.Lo < B.Lo)) {return -1;}
	return 0;
}
int Cmp(uint192 A, uint128 B) {
	if (A.H != 0) {
		return 1;
	}
	return Cmp(A.L, B);
}
int Cmp(uint192 A, uint192 B) {
	if (A.H == B.H) {
		return Cmp(A.L, B.L);
	}
	if (A.H > B.H) {
		return 1;
	}
	if (A.H < B.H) {
		return -1;
	}
}
int Cmp(uint256 A, uint128 B) {
	if (A.H.Hi != 0 || A.H.Lo != 0) {
		return 1;
	}
	return Cmp(A.L, B);
}
int Cmp(uint256 A, uint256 B) {
	int d = Cmp(A.H, B.H);
	if (d != 0) { return d; }
	return Cmp(A.L, B.L);
}
void Inc(inout uint128 A) {
	A.Lo += 1;
	if (A.Lo == 0) {
		A.Hi += 1;
	}
}
void Sub(inout uint128 A, uint128 B) {
	uint64_t x = A.Lo - B.Lo;
	uint64_t C = (((x & B.Lo) & 1l) + (B.Lo >> 1) + (x >> 1)) >> 63;
	A.Hi = A.Hi - (B.Hi + C);
	A.Lo = x;
}
void Sub(inout uint256 A, uint256 B) {
	uint64_t x = A.L.Lo - B.L.Lo;
	uint C = 0;
	if (x > A.L.Lo) {C++;}
	A.L.Lo = x;
	x = A.L.Hi - (B.L.Hi + C);
	C = 0;
	if (x > A.L.Hi) {C++;} 
	A.L.Hi = x;

	x = A.H.Lo - (B.H.Lo + C);
	C = 0;
	if (x > A.H.Lo) {C++;}
	A.H.Lo = x;

	A.H.Hi = A.H.Hi - (B.H.Hi + C);
}
void Sub(inout uint192 A, uint192 B) {
	uint64_t x = A.L.Lo - B.L.Lo;
	uint C = 0;
	if (x > A.L.Lo) {C++;}
	A.L.Lo = x;
	x = A.L.Hi - (B.L.Hi + C);
	C = 0;
	if (x > A.L.Hi) {C++;} 
	A.L.Hi = x;

	A.H = A.H - (B.H + C);
}
void Add(inout uint128 A, uint128 B) {
	uint64_t x = A.Lo + B.Lo;
	uint64_t C = 0;
	if (x < A.Lo) {C++;}
	A.Hi = A.Hi + (B.Hi + C);
	A.Lo = x;
}
// Add two uint256 values
void Add256(inout uint256 A, uint256 B) {
    uint64_t r = A.L.Lo + B.L.Lo;
    uint64_t c = (r < A.L.Lo) ? 1 : 0;
    A.L.Lo = r;
    
    r = A.L.Hi + B.L.Hi + c;
    c = (r < A.L.Hi || (c == 1 && r == A.L.Hi)) ? 1 : 0;
    A.L.Hi = r;
    
    r = A.H.Lo + B.H.Lo + c;
    c = (r < A.H.Lo || (c == 1 && r == A.H.Lo)) ? 1 : 0;
    A.H.Lo = r;
    
    A.H.Hi += B.H.Hi + c;
}

bool Zero(uint128 A) {
	return A.Lo == 0 && A.Hi == 0;
}
void multiply64to128(uint64_t x, uint64_t y, out uint128 r)
{
	uint64_t lowbits = 0xfffffffful;
	u64vec4 x4 = u64vec4(x >> 32, x & lowbits, x >> 32, x & lowbits);
	u64vec4 y4 = u64vec4(y >> 32, y >> 32, y & lowbits, y & lowbits);
	u64vec4 p = x4 * y4;

	uint64_t middle = p[2] + (p[3] >> 32) + (p[1] & lowbits);
	r.Hi = p[0] + (middle >> 32) + (p[1] >> 32);
	r.Lo = (middle << 32) | (p[3] & lowbits);

	/*
	
	uint64_t p11 = (x >> 32) * (y >> 32);
	uint64_t p01 = (x & lowbits) * (y >> 32);
	uint64_t p10 = (x >> 32) * (y & lowbits);
	uint64_t p00 = (x & lowbits) * (y & lowbits);

	// 64-bit product + two 32-bit values
	uint64_t middle = p10 + (p00 >> 32) + (p01 & lowbits);

	// 64-bit product + two 32-bit values
	r.Hi = p11 + (middle >> 32) + (p01 >> 32);
	r.Lo = (middle << 32) | (p00 & lowbits);
	*/
}

void multiply64to128x(uint64_t lhs, uint64_t rhs, inout uint128 r) {

        // First calculate all of the cross products.
	uint64_t lo_lo = (lhs & 0xFFFFFFFFul) * (rhs & 0xFFFFFFFFul);
	uint64_t hi_lo = (lhs >> 32)        * (rhs & 0xFFFFFFFFul);
	uint64_t lo_hi = (lhs & 0xFFFFFFFFul) * (rhs >> 32);
	uint64_t hi_hi = (lhs >> 32)        * (rhs >> 32);

        // Now add the products together. These will never overflow.
	uint64_t cross = (lo_lo >> 32) + (hi_lo & 0xFFFFFFFFul) + lo_hi;
	uint64_t upper = (hi_lo >> 32) + (cross >> 32)        + hi_hi;

	r.Hi = upper;
	r.Lo = (cross << 32) | (lo_lo & 0xFFFFFFFFul);
}

void Mul128(uint128 A, uint64_t B, inout uint128 X) {
	multiply64to128(A.Lo, B, X);
	uint128 T;
	multiply64to128(A.Hi, B, T);
	X.Hi += T.Lo;
}


void Sq256(uint128 A, inout uint256 X) {
	multiply64to128(A.Hi, A.Hi, X.H);
	multiply64to128(A.Lo, A.Lo, X.L);

	uint128 T;
	multiply64to128(A.Hi, A.Lo, T);
	Lsh(T);

	X.L.Hi += T.Lo;
	if (X.L.Hi < T.Lo) {
		Inc(X.H);
	}
	X.H.Lo += T.Hi;
	if (X.H.Lo < T.Hi) {
		X.H.Hi++;
	}
}
void Sq192(uint128 A, inout uint192 X) {	
	X.H = A.Hi * A.Hi;
	multiply64to128(A.Lo, A.Lo, X.L);

	uint128 T;
	multiply64to128(A.Hi, A.Lo, T);
	Lsh(T);

	X.L.Hi += T.Lo;
	if (X.L.Hi < T.Lo) {
		X.H++;
	}
	X.H += T.Hi;
}
void Mul192(uint128 A, uint128 B, inout uint192 X) {
	X.H = A.Hi * B.Hi;
	multiply64to128(A.Lo, B.Lo, X.L);

	uint128 T;
	multiply64to128(B.Hi, A.Lo, T);

	X.L.Hi += T.Lo;
	if (X.L.Hi < T.Lo) {
		X.H++;
	}
	X.H += T.Hi;

	multiply64to128(A.Hi, B.Lo, T);

	X.L.Hi += T.Lo;
	if (X.L.Hi < T.Lo) {
		X.H++;
	}
	X.H += T.Hi;
}
void Mul256(uint128 A, uint128 B, inout uint256 X) {
	multiply64to128(A.Hi, B.Hi, X.H);
	multiply64to128(A.Lo, B.Lo, X.L);

	uint128 T;
	multiply64to128(B.Hi, A.Lo, T);

	X.L.Hi += T.Lo;
	if (X.L.Hi < T.Lo) {
		Inc(X.H);
	}
	X.H.Lo += T.Hi;
	if (X.H.Lo < T.Hi) {
		X.H.Hi++;
	}

	multiply64to128(A.Hi, B.Lo, T);

	X.L.Hi += T.Lo;
	if (X.L.Hi < T.Lo) {
		Inc(X.H);
	}
	X.H.Lo += T.Hi;
	if (X.H.Lo < T.Hi) {
		X.H.Hi++;
	}
}

// 2^64, 2^128 and 2^192
const 	double p64 =  18446744073709551616.0lf;
const	double p128 = 340282366920938463463374607431768211456.0lf;
const	double p192 = 6277101735386680763835789423207666416102355444464034512896.0lf;

double toF(uint128 A) {
	return double(A.Lo) + double(A.Hi) * p64;
}
double toF(uint192 A) {

	return double(A.L.Lo) + double(A.L.Hi)*p64 + double(A.H)*p128;
}
double toF(uint256 A) {

	return double(A.L.Lo) + double(A.L.Hi)*p64 + double(A.H.Lo)*p128 + double(A.H.Hi)*p192;
}
void fto128(double f, out uint128 A) {
	A.Hi = uint64_t(f / p64);
	A.Lo = uint64_t(f - double(A.Hi) * p64);
}


// ---------- 64-bit inverse mod 2^64 (Newton iteration), for odd a ----------
uint64_t InvMod2_64(uint64_t a) {
    // Returns a^{-1} mod 2^64 (a must be odd)
    uint64_t x = uint64_t(1);
    // Each iteration doubles correct bits: 1->2->4->8->16->32->64
    for (int i = 0; i < 6; i++) {
        x = x * (uint64_t(2) - a * x);
    }
    return x;
}

// n0' = -n0^{-1} mod 2^64, where n0 is the low limb of N
uint64_t MontN0Prime(uint128 N) {
    return uint64_t(0) - InvMod2_64(N.Lo);
}

// ---------- Montgomery reduction for 2-limb (128-bit) modulus ----------
void MontReduce2(uint256 T, uint128 N, uint64_t n0prime, out uint128 R) {
    // limbs: t0,t1,t2,t3 are 256-bit; keep one extra top limb t4 for carry safety
    uint64_t t0 = T.L.Lo;
    uint64_t t1 = T.L.Hi;
    uint64_t t2 = T.H.Lo;
    uint64_t t3 = T.H.Hi;
    uint64_t t4 = 0;

    for (int i = 0; i < 2; i++) {
        uint64_t m = t0 * n0prime; // (t0 * n0') mod 2^64 via wraparound

        uint128 p0, p1;
        multiply64to128(m, N.Lo, p0); // m*N0
        multiply64to128(m, N.Hi, p1); // m*N1

        // Add (m*N) into (t0..t4), aligned at limb 0:
        // contributes: p0.Lo -> t0, p0.Hi -> t1
        //              p1.Lo -> t1, p1.Hi -> t2
        uint64_t carry;

        // t0 += p0.Lo
        uint64_t s = t0 + p0.Lo;
        carry = (s < t0) ? 1ul : 0ul;
        t0 = s;

        // t1 += p0.Hi + p1.Lo + carry
        uint64_t c1, c2, c3;
        s = t1 + p0.Hi;  c1 = (s < t1) ? 1ul : 0ul;  t1 = s;
        s = t1 + p1.Lo;  c2 = (s < t1) ? 1ul : 0ul;  t1 = s;
        s = t1 + carry;  c3 = (s < t1) ? 1ul : 0ul;  t1 = s;
        carry = c1 + c2 + c3; // 0..2

        // t2 += p1.Hi + carry
        s = t2 + p1.Hi;  c1 = (s < t2) ? 1ul : 0ul;  t2 = s;
        s = t2 + carry;  c2 = (s < t2) ? 1ul : 0ul;  t2 = s;
        carry = c1 + c2; // 0..2

        // t3 += carry, propagate into t4
        s = t3 + carry;
        c1 = (s < t3) ? 1ul : 0ul;
        t3 = s;
        t4 += c1; // 0..1

        // Divide by b (shift right 64): drop t0
        t0 = t1;
        t1 = t2;
        t2 = t3;
        t3 = t4;
        t4 = 0;
    }

    // Now we have a 3-limb value: t0 + t1*b + t2*b^2 (t3==0)
    uint128 res; res.Lo = t0; res.Hi = t1;

    // If top limb nonzero OR res >= N, subtract N once (on the 3-limb number)
    if (t2 != 0 || Cmp(res, N) >= 0) {
        // subtract N from (t2:res.Hi:res.Lo)
        uint64_t lo = res.Lo;
        uint64_t hi = res.Hi;
        uint64_t top = t2;

        uint64_t x = lo - N.Lo;
        uint64_t borrow = (x > lo) ? 1ul : 0ul;
        lo = x;

        x = hi - N.Hi;
        uint64_t b2 = (x > hi) ? 1ul : 0ul;
        hi = x;

        if (borrow != 0) {
            x = hi - 1ul;
            b2 += (x > hi) ? 1ul : 0ul;
            hi = x;
        }

        top = top - b2; // should land at 0
        res.Lo = lo;
        res.Hi = hi;
    }

    // Safety (should be unnecessary, but cheap)
    if (Cmp(res, N) >= 0) {
        Sub(res, N);
    }

    R = res;
}

void MontMul(uint128 A, uint128 B, uint128 N, uint64_t n0prime, out uint128 R) {
    uint256 T;
    Mul256(A, B, T);
    MontReduce2(T, N, n0prime, R);
}

void MontSquare(inout uint128 A, uint128 N, uint64_t n0prime) {
    uint128 t;
    MontMul(A, A, N, n0prime, t);
    A = t;
}

// Multiply-by-2 in Montgomery form is just doubling mod N
void MontDouble(inout uint128 A, uint128 N) {
    Add(A, A);
    if (Cmp(A, N) >= 0) { Sub(A, N); }
}


void ReduceMod256(inout uint256 X, uint128 Q, double qinv) {
    uint256 Y;
    uint128 D;

    // it is faster to do this w/o testing, even if D might be zero and have no effect.
    double x = toF(X);
    fto128(x * qinv, D);
    Mul256(D, Q, Y);
    Sub(X, Y);

    // refine until < Q
    while (Cmp(X, Q) > 0) {
        x = toF(X);
        fto128(x * qinv, D);
        if (Zero(D)) { D.Lo = 1; }
        Mul256(D, Q, Y);
        Sub(X, Y);
    }
}

// oneBar = R mod Q, where R = 2^128 (Montgomery representation of 1)
void MontOne(uint128 Q, double qinv, out uint128 oneBar) {
    uint256 X;
    X.L.Lo = 0; X.L.Hi = 0;
    X.H.Lo = 1; X.H.Hi = 0; // X = 2^128
    ReduceMod256(X, Q, qinv);
    oneBar = X.L;
}


bool Exp2Equals1_Mont(uint64_t P, uint128 Q) {
    // qinv only used to compute R mod Q once; inner loop is pure Montgomery.
    double qinv = 0.99999999999999 / toF(Q);

    uint128 oneBar;
    MontOne(Q, qinv, oneBar);

    uint64_t n0prime = MontN0Prime(Q);

    uint128 A = oneBar; // starts at 1 in Montgomery form

    int top = int(findMSB(P));
    uint64_t one = uint64_t(1);

    for (int b = top; b >= 0; b--) {
        MontSquare(A, Q, n0prime);
        if ((P & (one << b)) != 0) {
            MontDouble(A, Q); // multiply-by-2 step
        }
    }

    // 2^P mod Q == 1  <=>  A == 1 (in Montgomery) <=> A == oneBar
    return (A.Lo == oneBar.Lo) && (A.Hi == oneBar.Hi);
}


uint Mod(uint128 X, uint Q) {
	if (X.Hi == 0) {
		return uint(X.Lo % Q);
	}
	uint128 Y, D;
	int i = 0;
	double qinv = 0.9999999999999 / double(Q);
	while (i < 10 && Cmp(X, Q) > 0) {
		double x = toF(X);
		double xqi = x * qinv;
		fto128(xqi, D);
		if (Zero(D)) {
			D.Lo = 1;
		}
		Mul128(D, Q, Y);
		Sub(X, Y);
		i++;
	}
	if (X.Lo == Q) return 0;
	return uint(X.Lo);
}

bool Mod0(uint128 X, uint64_t Q) {
	if (X.Hi == 0) {
		return (X.Lo % Q) == 0;
	}
	uint128 Y, D;
	int i = 0;
	double qinv = 0.9999999999999 / double(Q);
	while (i < 10 && Cmp(X, Q) > 0) {
		double x = toF(X);
		double xqi = x * qinv;
		fto128(xqi, D);
		if (Zero(D)) {
			D.Lo = 1;
		}
		Mul128(D, Q, Y);
		Sub(X, Y);
		i++;
	}
	return Zero(X) || X.Lo == Q;
}

// Montgomery version
bool tfm(uint128 k) {
	uint128 sq, q, r;

	Mul128(k, P, q);
	Lsh(q);
	Inc(q);

	return Exp2Equals1_Mont(P, q);
}


/*

// 256-bit Floating point version
void SqMod(inout uint128 A, uint128 Q, bool doshift, double qinv) {
	uint256 X, Y;
	uint128 D;

	Sq256(A, X);
	if (doshift) {
		Lsh(X);
	}

	//if (X.H.Hi > 0) {atomicAdd(Debug[0], 1);}

	// it is faster to do this w/o testing, even if D might be zero and have no effect.
	if (true) {
		double x = toF(X);
		fto128(x*qinv, D);
		Mul256(D, Q, Y);
		Sub(X, Y);

		x = toF(X);
		fto128(x*qinv, D);
		Mul256(D, Q, Y);
		Sub(X, Y);
	}
	//int i = 0; // limit to 10, in case we have a bug, don't want to get stuck...

	// Using floating point, take a guess at a number D such that we could subtract D*Q to leave our remander.
	// Always guess D a little low, and refine our guess as we get closer.
	// normally takes 2 passes.
	//while (i < 10 && Cmp(X, Q) > 0) {
	while (Cmp(X, Q) > 0) {

		double x = toF(X);
		fto128(x*qinv, D);
		if (Zero(D)) {
			D.Lo = 1;
		}		
		Mul256(D, Q, Y);

		Sub(X, Y);
		//i++;
	}
	//if (i > 3) {Debug[1]++;}
	A = X.L;
}
// 192-bit Floating point version
void SqMod9(inout uint128 A, uint128 Q, bool doshift, double qinv) {
	uint192 X, Y;
	uint128 D;

	Sq192(A, X);
	//Mul192(A, A, X);
	if (doshift) {
		Lsh(X);
	}
	// it is faster to do this w/o testing, even if D might be zero and have no effect.
	if (true) {
		double x = toF(X);
		fto128(x*qinv, D);
		Mul192(D, Q, Y);
		Sub(X, Y);

		x = toF(X);
		fto128(x*qinv, D);
		Mul192(D, Q, Y);
		Sub(X, Y);
	}

	//int i = 0; // limit to 10, in case we have a bug, don't want to get stuck...

	// Using floating point, take a guess at a number D such that we could subtract D*Q to leave our remander.
	// Always guess D a little low, and refine our guess as we get closer.
	// normally takes 2 passes.
	//
	//while (i < 10 && Cmp(X, Q) > 0) {
	while (Cmp(X, Q) > 0) {
		double x = toF(X);
		fto128(x*qinv, D);
		if (Zero(D)) {
			D.Lo = 1;
		}
		Mul192(D, Q, Y);
		Sub(X, Y);
		//i++;
	}
	//if (gl_GlobalInvocationID.x == 0 && i > 5) {Debug[1]++;}
	A = X.L;
}


// 256-bit version
bool tf(uint128 k) {
	uint128 sq, q;

	int top = int(findMSB(P));

	// q = 2 * p * k + 1
	//pp.Lo = P;
	//pp.Hi = 0;
	Mul128(k, P, q);
	Lsh(q);
	Inc(q);

        // Make our 1/q just a tiny bit too small, so we don't over estimate,
        // but not so small as to need extra passes.
	double qinv = 0.99999999999999 / toF(q);

	// Do the TF math: Starting with 1, repeatedly square,
	//  remove the top bit of the exponent and if 1 multiply squared value by 2,
	//  then compute mod Q.
	uint64_t one = 1L;
	sq.Lo = 1; sq.Hi = 0;
	sq = PreSq;
	for (int b = PreTop; b >= 0; b--) {
		bool bb = (P & (one << b)) != 0;
		SqMod(sq, q, bb, qinv);
	}
	// If the result is 1, then we found a factor.
	return sq.Lo == 1 && sq.Hi == 0;
}

// 192-bit version
bool tf9(uint128 k) {
	uint128 sq, q;

	int top = int(findMSB(P));

	// q = 2 * p * k + 1
	//pp.Lo = P;
	//pp.Hi = 0;
	Mul128(k, P, q);
	Lsh(q);
	Inc(q);

        // Make our 1/q just a tiny bit too small, so we don't over estimate,
        // but not so small as to need extra passes.
	double qinv = 0.99999999999999 / toF(q);

	// Do the TF math: Starting with 1, repeatedly square,
	//  remove the top bit of the exponent and if 1 multiply squared value by 2,
	//  then compute mod Q.
	uint64_t one = 1L;
	sq.Lo = 1; sq.Hi = 0;
	sq = PreSq;
	for (int b = PreTop; b >= 0; b--) {
		bool bb = (P & (one << b)) != 0;
		SqMod9(sq, q, bb, qinv);
	}
	// If the result is 1, then we found a factor.
	return sq.Lo == 1 && sq.Hi == 0;
}

//
// The squaring rounds are all the same, until Sq>Q.  So we can pre-compute those.
//
void pretf(uint128 k) {
	uint128 sq, q;

	int top = int(findMSB(P));

	// q = 2 * p * k + 1
	//pp.Lo = P;
	//pp.Hi = 0;
	Mul128(k, P, q);
	Lsh(q);
	Inc(q);

        // Make our 1/q just a tiny bit too small, so we don't over estimate,
        // but not so small as to need extra passes.
	double qinv = 0.99999999999999 / toF(q);

	uint256 X;
	// Do the TF math: Starting with 1, repeatedly square,
	//  remove the top bit of the exponent and if 1 multiply squared value by 2,
	//  then compute mod Q.
	uint64_t one = 1L;
	sq.Lo = 1; sq.Hi = 0;
	for (int b = top; b >= 0; b--) {
		bool bb = (P & (one << b)) != 0;
		Sq256(sq, X);
		if (bb) {
			Lsh(X);
		}
		if (Cmp(X, q) > 0) {
			break;
		}
		sq = X.L;
		PreTop = b-1;
		PreSq = sq;
	}
}
*/
void main() {
	uint I = gl_GlobalInvocationID.x;

	// copy some counters back to shared memory, just for sanity checks.
	if (Init == 5) {
		if (I == 0) {
			Debug[0] = xLl;
			Debug[1] = xL2;
		}
		return;
	}
	// initialize atomic counters for the next invocation.
	if (Init >= 10) {
		if (I == 0) {
			Debug[0] = 0;
			Debug[1] = 0;
			Debug[2] = 0;
			Debug[3] = 0;
			xL = 0;
			L3 = 0;
			NFound = 0;
			if (Init == 12) {
				xL2 = 0;
			}
			if (Init == 11) {
				xLl = 0;
			}
		}
		return;
	}
	// TF run
	if (Init == 0) {
		// while (Big == 0) {
		while (true) {
			//
			// K is the base starting value for this invocation. 
			//
			uint128 k;
			k.Lo = K[0];
			k.Hi = K[1];

			// get the next index from the List to test
			uint i = atomicAdd(xL, 1);
			if (i >= xL2) {
				// All threads return here.
				return;
			}
			uint o = List2[i];
			uint128 oo;

			// offset from the base K
			oo.Lo = o;
			oo.Hi = 0;

			// the actual K this thread will test.
			Add(k, oo);

			if (Zero(k)) {continue;}

			
			//if (tf9(k)) {  // 96/192-bit math
			if (tfm(k)) {    // Montgomery version
				// How many have we found?
				uint f = atomicAdd(NFound, 1);
				// return the 128-bit K values
				Found[f][0] = k.Lo;
				Found[f][1] = k.Hi;
			}
		}
		// no threads reach here.
	}

 	// perform the second sieve, populating list2.  Called before each call to Init==0
	if (Init == 2) {
		uint128 k;
		k.Lo = K[0];
		k.Hi = K[1];
		//if (I == 0) {
		//	uint128 kt = k;
		//	Inc(kt);  // incase of 0.
		//	pretf(kt);
		//	//return;
		//}
		const uint lim = MnLen - 0;
		uint kmod[lim];
		for (int j = 0; j < lim; j++) {
			kmod[j] = Mod(k, Mn[j]);
		}
		while (true) {
			uint i = atomicAdd(xL, 1);
			uint o = List[i];
			if (i >= xLl) { return; }

			bool cc = true;
			for (int j = 0; j < lim; j++) {
				uint ix = (o + kmod[j])%Mn[j];
				bool cx = 0 == (Xx[j][ix/32] & (1u << (ix%32)));
				//if (!cx) {cc = false;}
				cc = cc && cx;
			}
			if (cc)
			{
				uint ii = atomicAdd(xL2, 1);
				List2[ii] = o;
			} else if (false) {
				// Just for debugging, normally not used.
				// Give a sample of composites back to the CPU for primality testing.
				// This is just for looking for bugs in the sieving process.
			     	uint ii = atomicAdd(L3, 1);
				if (ii < 1000) {
					Test[ii] = o;
				}
			}
		}
		// no threads reach here
		//return;
	}
	// initialize our bit arrays to zero.  Only called once, before Init==1
	if (Init == 3) {
		while (true) {
			uint i = atomicAdd(xL, 1);
			if (i >= 1+Mn[6]/32) {
				// All threads return here.
				return;
			}
			for (int j = 0; j < MnLen; j++) {
				Xx[j][i] = 0;
			}
		}
	}
	// perform the first sieve
	if (Init == 1) {
		uint once = 0;
		while (true) {
			uint i = atomicAdd(xL, 1);
			if (i >= M) {
				// All threads return here.
				return;
			}
			if (once == 0) {
				once = 1;
				// for debugging, count how many threads got scheduled.
				// the atomic op is a little bit expensive, but only the first time in the while loop.
				atomicAdd(Debug[2], 1);
			}
			uint128 Q;
			multiply64to128(P, uint64_t(i), Q);
			Lsh(Q);
			Inc(Q);

			uint64_t qa7 = Q.Lo & 7;
			bool y = (qa7 == 3) || (qa7 == 5) || Mod0(Q,3) ||
				Mod0(Q,5) || Mod0(Q,7) || Mod0(Q,11) || Mod0(Q,13) ||
				Mod0(Q,17) || Mod0(Q,19) || Mod0(Q,23);

			if (!y) {
				uint o = atomicAdd(xLl, 1);
				List[o] = i;
			}
			// the mess below is apparently difficult to schedule, but we don't really care,
			// this is only called once per exponent
			if (i < Mn[2]) {
				bool c = (Mod0(Q,29) || Mod0(Q,31) || Mod0(Q,37)|| Mod0(Q,41) || Mod0(Q,43));
				if (c) {
					atomicOr(Xx[2][i/32], 1u << (i%32));
				}
			}
			if (i < Mn[3]) {
				bool c = (Mod0(Q,47) || Mod0(Q,53) || Mod0(Q,59)|| Mod0(Q,61));
				if (c) {
					atomicOr(Xx[3][i/32], 1u << (i%32));
				}
			}
		       	if (i < Mn[4]) {			     
				bool c = (Mod0(Q,67) || Mod0(Q,71) || Mod0(Q,73)|| Mod0(Q,79));
				if (c) {
					atomicOr(Xx[4][i/32], 1u << (i%32));
				}
			}
			if (i < Mn[5]) {			     
				bool c = (Mod0(Q,83) || Mod0(Q,89) || Mod0(Q,97)|| Mod0(Q,101));
				if (c) {
					atomicOr(Xx[5][i/32], 1u << (i%32));
				}
			}
			if (i < Mn[6]) {			     
				bool c = (Mod0(Q,103)||Mod0(Q,107) || Mod0(Q,109)||Mod0(Q,113));
				if (c) {
					atomicOr(Xx[6][i/32], 1u << (i%32));
				}
			}
			if (i < Mn[7]) {
				bool c = (Mod0(Q,127)||Mod0(Q,131) || Mod0(Q,137));
				if (c) {
					atomicOr(Xx[7][i/32], 1u << (i%32));
				}
			}
			if (i < Mn[8]) {
				bool c = (Mod0(Q,149)||Mod0(Q,151) || Mod0(Q,157));
				if (c) {
					atomicOr(Xx[8][i/32], 1u << (i%32));
				}
			}
			if (i < Mn[9]) {
				bool c = (Mod0(Q,163)||Mod0(Q,167) || Mod0(Q,173));
				if (c) {
					atomicOr(Xx[9][i/32], 1u << (i%32));
				}
			}
			if (i < Mn[0]) {
				bool c = (Mod0(Q,179)||Mod0(Q,181) || Mod0(Q,191));
				if (c) {
					atomicOr(Xx[0][i/32], 1u << (i%32));
				}
			}
			if (i < Mn[1]) {
				bool c = (Mod0(Q,193)||Mod0(Q,197) || Mod0(Q,199));
				if (c) {
					atomicOr(Xx[1][i/32], 1u << (i%32));
				}
			}
			if (i < Mn[10]) {
				bool c = (Mod0(Q,211)||Mod0(Q,223) || Mod0(Q,227));
				if (c) {
					atomicOr(Xx[10][i/32], 1u << (i%32));
				}
			}
			if (i < Mn[11]) {
				bool c = (Mod0(Q,229)||Mod0(Q,233) || Mod0(Q,239));
				if (c) {
					atomicOr(Xx[11][i/32], 1u << (i%32));
				}
			}
			if (i < Mn[12]) {
				bool c = (Mod0(Q,241)||Mod0(Q,251) || Mod0(Q,257));
				if (c) {
					atomicOr(Xx[12][i/32], 1u << (i%32));
				}
			}
			if (i < Mn[13]) {
				bool c = (Mod0(Q,139)||Mod0(Q,263) || Mod0(Q,269));
				if (c) {
					atomicOr(Xx[13][i/32], 1u << (i%32));
				}
			}
			if (i < Mn[14]) {
				bool c = (Mod0(Q,271)||Mod0(Q,277) || Mod0(Q,281));
				if (c) {
					atomicOr(Xx[14][i/32], 1u << (i%32));
				}
			}
			if (i < Mn[15]) {
				bool c = (Mod0(Q,283)||Mod0(Q,293) || Mod0(Q,307));
				if (c) {
					atomicOr(Xx[15][i/32], 1u << (i%32));
				}
			}
			if (i < Mn[16]) {
				bool c = (Mod0(Q,311)||Mod0(Q,313) || Mod0(Q,317));
				if (c) {
					atomicOr(Xx[16][i/32], 1u << (i%32));
				}
			}
			if (i < Mn[17]) {
				bool c = (Mod0(Q,331)||Mod0(Q,337) || Mod0(Q,347));
				if (c) {
					atomicOr(Xx[17][i/32], 1u << (i%32));
				}
			}
			if (i < Mn[18]) {
				bool c = (Mod0(Q,349)||Mod0(Q,353) || Mod0(Q,359));
				if (c) {
					atomicOr(Xx[18][i/32], 1u << (i%32));
				}
			}
			if (i < Mn[19]) {
				bool c = (Mod0(Q,367)||Mod0(Q,373) || Mod0(Q,379));
				if (c) {
					atomicOr(Xx[19][i/32], 1u << (i%32));
				}
			}
			if (i < Mn[20]) {
				bool c = (Mod0(Q,383)||Mod0(Q,389) || Mod0(Q,397));
				if (c) {
					atomicOr(Xx[20][i/32], 1u << (i%32));
				}
			}
			if (i < Mn[21]) {
				bool c = (Mod0(Q,401)||Mod0(Q,409) || Mod0(Q,419));
				if (c) {
					atomicOr(Xx[21][i/32], 1u << (i%32));
				}
			}/*
			if (i < Mn[22]) {
				bool c = (Mod0(Q,421)||Mod0(Q,431) || Mod0(Q,433));
				if (c) {
					atomicOr(Xx[22][i/32], 1u << (i%32));
				}
			}*/
		}
		// no threads reach here.
	}
}
