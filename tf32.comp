#version 460

#extension GL_EXT_shader_explicit_arithmetic_types_int64 : enable
//#extension GL_EXT_shader_atomic_int64: enable
#extension GL_GOOGLE_include_directive : enable

#include "common.glsl"

struct uint96 {
	uvec3 x;
};

struct uint192 {
	uint96 H;
	uint96 L;
};


bool Zero(uint96 A) {
	return A.x[0] == 0 && A.x[1] == 0 && A.x[2] == 0;
}
bool Zero(uint192 A) {
	return Zero(A.L) && Zero(A.H);
}

void Lsh(inout uint96 A) {
	uint c0 = A.x[0] & (uint(1) << 31);
	uint c1 = A.x[1] & (uint(1) << 31);
	A.x[0] <<= 1;
	A.x[1] <<= 1;
	A.x[1] |= c0 >> 31;
	A.x[2] <<= 1;
	A.x[2] |= c1 >> 31;
}
void Lsh(inout uint192 A) {
	
	uint c = A.L.x[2] & (uint(1) << 31);
	Lsh(A.L);
	Lsh(A.H);
	A.H.x[0] |= c >> 31;
}
int Cmp(uint96 A, uint B) {
	if (A.x[2] > 0) {return 1;}
	if (A.x[1] > 0) {return 1;}
	if (A.x[0] > B) {return 1;}
	if (A.x[0] < B) {return -1;}
	return 0;
}
int Cmp(uint96 A, uint96 B) {
	if (A.x[2] > B.x[2]) {return 1;}
	if (A.x[2] < B.x[2]) {return -1;}
	if (A.x[1] > B.x[1]) {return 1;}
	if (A.x[1] < B.x[1]) {return -1;}
	if (A.x[0] > B.x[0]) {return 1;}
	if (A.x[0] < B.x[0]) {return -1;}
	return 0;
}
int Cmp(uint192 A, uint96 B) {
	if (Zero(A.H)) {
		return Cmp(A.L, B);
	}
 	return 1;
}
int Cmp(uint192 A, uint192 B) {
	int c = Cmp(A.H, B.H);
	if (c != 0) {
		return c;
	}
	return Cmp(A.L, B.L);
}
void Inc(inout uint96 A) {
	A.x[0] += 1;
	if (A.x[0] == 0) {
		A.x[1] += 1;
		if (A.x[1] == 0) {
			A.x[2] += 1;
		}
	}
}
void Inc(inout uint192 A) {
	Inc(A.L);
	if (Zero(A.L)) {
		Inc(A.H);
	}
}
void Add(inout uint96 A, uint B) {
	uint C;
	A.x[0] = uaddCarry(A.x[0], B, C);
	A.x[1] = uaddCarry(A.x[1], C, C);
	A.x[2] += C;
}
void Add(inout uint96 A, uint96 B) {
	uint c, c1;
	A.x[0] = uaddCarry(A.x[0], B.x[0], c);
	A.x[1] = uaddCarry(A.x[1], c, c1);
	A.x[1] = uaddCarry(A.x[1], B.x[1], c);
	A.x[2] = A.x[2] + B.x[2] + c + c1;

}

void Sub(inout uint192 A, uint192 B) {

	uint c, c1;
	
	A.L.x[0] = usubBorrow(A.L.x[0], B.L.x[0], c);
	A.L.x[1] = usubBorrow(A.L.x[1], c, c1);
	A.L.x[1] = usubBorrow(A.L.x[1], B.L.x[1], c);

	A.L.x[2] = usubBorrow(A.L.x[2], c+c1, c);
	A.L.x[2] = usubBorrow(A.L.x[2], B.L.x[2], c1);

	A.H.x[0] = usubBorrow(A.H.x[0], c+c1, c);
	A.H.x[0] = usubBorrow(A.H.x[0], B.H.x[0], c1);	

	A.H.x[1] = usubBorrow(A.H.x[1], c+c1, c);	
	A.H.x[1] = usubBorrow(A.H.x[1], B.H.x[1], c1);	

	A.H.x[2] = usubBorrow(A.H.x[2], B.H.x[2]+c+c1, c);	
}
void Sub(inout uint96 A, uint96 B) {

	uint c, c1;
	
	A.x[0] = usubBorrow(A.x[0], B.x[0], c);
	A.x[1] = usubBorrow(A.x[1], c, c1);
	A.x[1] = usubBorrow(A.x[1], B.x[1], c);

	A.x[2] = usubBorrow(A.x[2], c+c1, c);
	A.x[2] = usubBorrow(A.x[2], B.x[2], c1);
}

void Mul192(inout uint96 A, uint96 B, out uint192 R)
{
	uint h[9], l[9];
	uint c1, c2, c;

	umulExtended(A.x[0], B.x[0], h[0], l[0]);
	umulExtended(A.x[0], B.x[1], h[1], l[1]);
	umulExtended(A.x[0], B.x[2], h[2], l[2]);
	umulExtended(A.x[1], B.x[0], h[3], l[3]);
	umulExtended(A.x[1], B.x[1], h[4], l[4]);
	umulExtended(A.x[1], B.x[2], h[5], l[5]);
	umulExtended(A.x[2], B.x[0], h[6], l[6]);
	umulExtended(A.x[2], B.x[1], h[7], l[7]);
	umulExtended(A.x[2], B.x[2], h[8], l[8]);

	R.L.x[0] = l[0];
	R.L.x[1] = uaddCarry(h[0], l[1], c1);	c = c1;
	R.L.x[1] = uaddCarry(R.L.x[1], l[3], c1);	c += c1;

	R.L.x[2] = uaddCarry(l[2], c, c1);	c = c1;
	R.L.x[2] = uaddCarry(R.L.x[2], h[1], c1);	c += c1;
	R.L.x[2] = uaddCarry(R.L.x[2], h[3], c1);	c += c1;
	R.L.x[2] = uaddCarry(R.L.x[2], l[4], c1);	c += c1;
	R.L.x[2] = uaddCarry(R.L.x[2], l[6], c1);	c += c1;

	R.H.x[0] = uaddCarry(l[5], c, c1);	c = c1;
	R.H.x[0] = uaddCarry(R.H.x[0], h[2], c1);	c += c1;
	R.H.x[0] = uaddCarry(R.H.x[0], h[4], c1);	c += c1;
	R.H.x[0] = uaddCarry(R.H.x[0], l[7], c1);	c += c1;
	R.H.x[0] = uaddCarry(R.H.x[0], h[6], c1);	c += c1;

	R.H.x[1] = uaddCarry(h[5], c, c1);	c = c1;
	R.H.x[1] = uaddCarry(R.H.x[1], l[8], c1);	c += c1;
	R.H.x[1] = uaddCarry(R.H.x[1], h[7], c1);	c += c1;
	R.H.x[2] = uaddCarry(h[8], c, c1);
}

void to96(uint64_t n, inout uint96 o) {
	uint p0 = uint(n);      // lower 32-bits
	uint p1 = uint(n>>32);  // upper 32-bits
	o.x = uvec3(p0, p1, 0);
}



// -------------------- Montgomery (b = 2^32) for uint96 modulus --------------------

// avoiding uint64_t.  could be slightly slower.
void xSub96Wide(inout uint96 A, inout uint top, uint96 B)
{
    uint borrow = 0u;

    // limb 0
    uint r0 = usubBorrow(A.x[0], B.x[0], borrow);

    // limb 1: subtract (B1 + borrow)
    uint carry = 0u;
    uint t1 = uaddCarry(B.x[1], borrow, carry);   // t1 = B1 + borrow (mod 2^32), carry = overflow
    uint b1 = 0u;
    uint r1 = usubBorrow(A.x[1], t1, b1);
    borrow = b1 | carry;                          // if carry==1, we effectively borrowed 2^32

    // limb 2: subtract (B2 + borrow)
    carry = 0u;
    uint t2 = uaddCarry(B.x[2], borrow, carry);
    uint b2 = 0u;
    uint r2 = usubBorrow(A.x[2], t2, b2);
    borrow = b2 | carry;

    top -= borrow;                                // borrow out of bit 95..0 borrows from 2^96 limb
    A.x = uvec3(r0, r1, r2);
}

// using uint64_t
void Sub96Wide(inout uint96 A, inout uint top, uint96 B) {
    uint64_t a0 = uint64_t(A.x[0]), a1 = uint64_t(A.x[1]), a2 = uint64_t(A.x[2]);
    uint64_t b0 = uint64_t(B.x[0]), b1 = uint64_t(B.x[1]), b2 = uint64_t(B.x[2]);

    uint64_t r0 = a0 - b0;
    uint64_t borrow = (a0 < b0) ? 1ul : 0ul;

    uint64_t t = b1 + borrow;
    uint64_t r1 = a1 - t;
    borrow = (a1 < t) ? 1ul : 0ul;

    t = b2 + borrow;
    uint64_t r2 = a2 - t;
    borrow = (a2 < t) ? 1ul : 0ul;

    top -= uint(borrow);          // top is 0/1; if borrow==1, we borrow from the 2^96 limb
    A.x = uvec3(uint(r0), uint(r1), uint(r2));
}


// 32-bit inverse mod 2^32 (Newton iteration), for odd a
uint InvMod2_32(uint a) {
    // Returns a^{-1} mod 2^32 (a must be odd)
    uint x = 1u;
    // bits: 1->2->4->8->16->32  (5 iterations)
    for (int i = 0; i < 5; i++) {
        x = x * (2u - a * x);
    }
    return x;
}

// n0' = -n0^{-1} mod 2^32, where n0 is the low limb of N
uint MontN0Prime(uint96 N) {
    return 0u - InvMod2_32(N.x[0]);
}

bool Eq96(uint96 A, uint96 B) {
    return all(equal(A.x, B.x));
}


// Subtract (B) from (top:A) where top is the extra 4th limb.
// Assumes (top:A) >= B.
// using uint, seems to be faster
void Sub96Top(inout uint96 A, inout uint top, uint96 B)
{
    uint borrow = 0u;

    // limb 0: A0 - B0
    uint r0 = usubBorrow(A.x[0], B.x[0], borrow);

    // limb 1: A1 - (B1 + borrow)
    uint carry = 0u;
    uint t1 = uaddCarry(B.x[1], borrow, carry);
    uint b1 = 0u;
    uint r1 = usubBorrow(A.x[1], t1, b1);
    borrow = b1 | carry;

    // limb 2: A2 - (B2 + borrow)
    carry = 0u;
    uint t2 = uaddCarry(B.x[2], borrow, carry);
    uint b2 = 0u;
    uint r2 = usubBorrow(A.x[2], t2, b2);
    borrow = b2 | carry;

    // top limb: top - borrow
    uint b3 = 0u;
    uint r3 = usubBorrow(top, borrow, b3); // borrow is 0/1 here
    // If b3 == 1, you underflowed the 128-bit value; that should not happen
    // if callers maintain the invariant that (top:A) >= B.

    A.x = uvec3(r0, r1, r2);
    top = r3;
}

// Subtract (B) from (top:A) where top is the extra 4th limb.
// Assumes (top:A) >= B.
// using uint64_t, seems to be slower
void xSub96Top(inout uint96 A, inout uint top, uint96 B) {
    uint64_t a0 = uint64_t(A.x[0]);
    uint64_t a1 = uint64_t(A.x[1]);
    uint64_t a2 = uint64_t(A.x[2]);
    uint64_t a3 = uint64_t(top);

    uint64_t b0 = uint64_t(B.x[0]);
    uint64_t b1 = uint64_t(B.x[1]);
    uint64_t b2 = uint64_t(B.x[2]);

    uint64_t r0 = a0 - b0;
    uint64_t borrow = (a0 < b0) ? 1ul : 0ul;

    uint64_t t = b1 + borrow;
    uint64_t r1 = a1 - t;
    borrow = (a1 < t) ? 1ul : 0ul;

    t = b2 + borrow;
    uint64_t r2 = a2 - t;
    borrow = (a2 < t) ? 1ul : 0ul;

    // top -= borrow
    uint64_t r3 = a3 - borrow;

    A.x[0] = uint(r0);
    A.x[1] = uint(r1);
    A.x[2] = uint(r2);
    top    = uint(r3);
}



uint add3c(uint a, uint b, uint c, inout uint carry)
{
    uint c1 = 0u, c2 = 0u;
    uint s  = uaddCarry(a, b, c1);
    s       = uaddCarry(s, c, c2);
    carry  += c1 + c2; // carry is a small uint; weâ€™ll normalize below
    return s;
}

uint add4c(uint a, uint b, uint c, uint d, inout uint carry)
{
    uint c1 = 0u, c2 = 0u, c3 = 0u;
    uint s  = uaddCarry(a, b, c1);
    s       = uaddCarry(s, c, c2);
    s       = uaddCarry(s, d, c3);
    carry  += c1 + c2 + c3;
    return s;
}
void MontReduce3(uint192 T, uint96 N, uint n0prime, out uint96 R)
{
    uint t0 = T.L.x[0];
    uint t1 = T.L.x[1];
    uint t2 = T.L.x[2];
    uint t3 = T.H.x[0];
    uint t4 = T.H.x[1];
    uint t5 = T.H.x[2];
    uint t6 = 0u;

    for (int i = 0; i < 3; i++) {
        uint m = t0 * n0prime; // mod 2^32 by wraparound

        uint hi0, lo0, hi1, lo1, hi2, lo2;
        umulExtended(m, N.x[0], hi0, lo0);
        umulExtended(m, N.x[1], hi1, lo1);
        umulExtended(m, N.x[2], hi2, lo2);

        // Add (m*N) into (t0..t6) using only 32-bit adds with carry.
        uint carry = 0u;

        // limb 0: t0 += lo0
        t0 = add3c(t0, lo0, 0u, carry);

        // limb 1: t1 += hi0 + lo1 + carry_in
        uint cin = carry; carry = 0u;
        t1 = add4c(t1, hi0, lo1, cin, carry);

        // limb 2: t2 += hi1 + lo2 + carry_in
        cin = carry; carry = 0u;
        t2 = add4c(t2, hi1, lo2, cin, carry);

        // limb 3: t3 += hi2 + carry_in
        cin = carry; carry = 0u;
        t3 = add3c(t3, hi2, cin, carry);

        // limb 4: t4 += carry_in
        cin = carry; carry = 0u;
        t4 = add3c(t4, cin, 0u, carry);

        // limb 5: t5 += carry_in
        cin = carry; carry = 0u;
        t5 = add3c(t5, cin, 0u, carry);

        // limb 6: t6 += carry_out (carry is 0/1 here, but keep it general)
        // We need to propagate if t6 overflows.
        uint c6 = 0u;
        t6 = uaddCarry(t6, carry, c6);
        // If c6 != 0, that would mean overflow beyond t6; should not happen for 3-limb Montgomery,
        // but you could track it if you ever expand to wider.

        // Divide by b (shift right 32): drop t0
        t0 = t1;
        t1 = t2;
        t2 = t3;
        t3 = t4;
        t4 = t5;
        t5 = t6;
        t6 = 0u;
    }

    uint top = t3;
    uint96 res;
    res.x = uvec3(t0, t1, t2);

    if (top != 0u || Cmp(res, N) >= 0) {
        Sub96Top(res, top, N);
    }

    if (Cmp(res, N) >= 0) {
        uint z = 0u;
        Sub96Top(res, z, N);
    }

    R = res;
}

// Reduce a 192-bit T modulo N using Montgomery (N is 96-bit, odd)
void xxMontReduce3(uint192 T, uint96 N, uint n0prime, out uint96 R) {
    // T as 6 limbs t0..t5 (little endian)
    uint t0 = T.L.x[0];
    uint t1 = T.L.x[1];
    uint t2 = T.L.x[2];
    uint t3 = T.H.x[0];
    uint t4 = T.H.x[1];
    uint t5 = T.H.x[2];
    uint t6 = 0u; // extra carry limb

    for (int i = 0; i < 3; i++) {
        uint m = t0 * n0prime; // mod 2^32 by wraparound

        uint hi0, lo0, hi1, lo1, hi2, lo2;
        umulExtended(m, N.x[0], hi0, lo0);
        umulExtended(m, N.x[1], hi1, lo1);
        umulExtended(m, N.x[2], hi2, lo2);

        // Add (m*N) into (t0..t6), aligned at limb 0, using 64-bit accumulators
        uint64_t acc;
        uint64_t carry;

        acc = uint64_t(t0) + uint64_t(lo0);
        t0 = uint(acc);
        carry = acc >> 32;

        acc = uint64_t(t1) + uint64_t(hi0) + uint64_t(lo1) + carry;
        t1 = uint(acc);
        carry = acc >> 32;

        acc = uint64_t(t2) + uint64_t(hi1) + uint64_t(lo2) + carry;
        t2 = uint(acc);
        carry = acc >> 32;

        acc = uint64_t(t3) + uint64_t(hi2) + carry;
        t3 = uint(acc);
        carry = acc >> 32;

        acc = uint64_t(t4) + carry;
        t4 = uint(acc);
        carry = acc >> 32;

        acc = uint64_t(t5) + carry;
        t5 = uint(acc);
        carry = acc >> 32;

        t6 += uint(carry);

        // Divide by b (shift right 32): drop t0
        t0 = t1;
        t1 = t2;
        t2 = t3;
        t3 = t4;
        t4 = t5;
        t5 = t6;
        t6 = 0u;
    }

    // Candidate is (t3: t2:t1:t0) but we only keep low 96 bits in R; t3 is "extra top" limb.
    uint top = t3;
    uint96 res;
    res.x = uvec3(t0, t1, t2);

    // If top != 0 OR res >= N, subtract N once
    if (top != 0u || Cmp(res, N) >= 0) {
        Sub96Top(res, top, N); // top should become 0
    }

    // Safety: ensure fully reduced
    if (Cmp(res, N) >= 0) {
        uint z = 0u;
        Sub96Top(res, z, N);
    }

    R = res;
}

void MontMul96(uint96 A, uint96 B, uint96 N, uint n0prime, out uint96 R) {
    uint192 T;
    Mul192(A, B, T);
    MontReduce3(T, N, n0prime, R);
}

void MontSquare96(inout uint96 A, uint96 N, uint n0prime) {
    uint96 t;
    MontMul96(A, A, N, n0prime, t);
    A = t;
}

// Multiply-by-2 in Montgomery form is just doubling mod N
void MontDouble96(inout uint96 A, uint96 N) {
    uint64_t acc = uint64_t(A.x[0]) + uint64_t(A.x[0]);
    A.x[0] = uint(acc);
    uint64_t carry = acc >> 32;

    acc = uint64_t(A.x[1]) + uint64_t(A.x[1]) + carry;
    A.x[1] = uint(acc);
    carry = acc >> 32;

    acc = uint64_t(A.x[2]) + uint64_t(A.x[2]) + carry;
    A.x[2] = uint(acc);
    // carry beyond 96 bits implies A >= 2^96, definitely >= N
    if ((acc >> 32) != 0ul || Cmp(A, N) >= 0) {
        // subtract N once
        uint z = 0u;
        Sub96Top(A, z, N);
    }
}

#ifndef NO_DOUBLE
// 2^64, 2^128 and 2^192
const   double p32 =  4294967296.0lf;
const 	double p64 =  18446744073709551616.0lf;
const   double p96 =  79228162514264337593543950336.0lf;
const	double p128 = 340282366920938463463374607431768211456.0lf;
const   double p160 = 1461501637330902918203684832716283019655932542976.0lf;
//const	double p192 = 6277101735386680763835789423207666416102355444464034512896.0lf;

double toF(uint96 A) {
	return double(A.x[0]) + double(A.x[1]) * p32 + double(A.x[2]) * p64;
}
double toF(uint192 A) {
	return double(A.L.x[0]) + double(A.L.x[1]) * p32 + double(A.L.x[2]) * p64 +
		double(A.H.x[0]) * p96 + double(A.H.x[1]) * p128 + double(A.H.x[2]) * p160;
}
void fto96x(double f, out uint96 A) {
	A.x[2] = uint(f / p64);
	A.x[1] = uint((f - double(A.x[2]) * p64) / p32);
	A.x[0] = uint(f - (double(A.x[2]) * p64 + double(A.x[1]) * p32));
}
void fto96(double f, out uint96 A) {
	uint x = uint(f / p64);
	A.x[2] = x;
	uint y = uint((f - x * p64) / p32);
	A.x[1] = y;
	A.x[0] = uint(f - (x * p64 + y * p32));
}

// old floating version, used for sieve.
uint fMod(uint96 X, uint Q) {
	if (X.x[2] == 0) {
		uint64_t x64 = X.x[0] | uint64_t(X.x[1]) << 32;
		return uint(x64 % Q);
	}
	//atomicAdd(Debug[0], 1);
	uint96 D, QQ;
	uint192 Y;
	int i = 0;
	double qinv = 0.9999999999999 / double(Q);
	QQ.x = uvec3(Q, 0, 0);
	while (i < 10 && Cmp(X, Q) > 0) {
		double x = toF(X);
		double xqi = x * qinv;
		fto96(xqi, D);
		if (Zero(D)) {
			D.x[0] = 1;
		}
		Mul192(D, QQ, Y);
		Sub(X, Y.L);
		i++;
	}
	if (X.x[0] == Q) return 0;
	return X.x[0];
}

//
// on a gpu with fast doubles, this is faster than MontOne96_Int()
//
void ReduceMod192(inout uint192 X, uint96 Q, double qinv) {
    uint192 Y;
    uint96 D;
/*
    // Two quick passes like old SqMod()
    fto96(toF(X) * qinv, D);
    Mul192(D, Q, Y);
    Sub(X, Y);

    fto96(toF(X) * qinv, D);
    Mul192(D, Q, Y);
    Sub(X, Y);
*/
    while (Cmp(X, Q) > 0) {
        fto96(toF(X) * qinv, D);
        if (Zero(D)) { D.x[0] = 1u; }
        Mul192(D, Q, Y);
        Sub(X, Y);
    }
}

// oneBar = R mod Q, where R = 2^96 (Montgomery representation of 1 for 96-bit modulus)
void MontOne96(uint96 Q, double qinv, out uint96 oneBar) {
    uint192 X;
    X.L.x = uvec3(0u, 0u, 0u);
    X.H.x = uvec3(1u, 0u, 0u); // X = 2^96
    ReduceMod192(X, Q, qinv);
    oneBar = X.L;
}
#else

#endif
// only used for sieving
uint Mod(uint96 X, uint Q)
{
    // Fast path: already fits in 64 bits.
    if (X.x[2] == 0u) {
	    uint64_t x64 = X.x[0] | uint64_t(X.x[1]) << 32;
	    return uint(x64 % Q);
    }

    // Compute (2^32 mod Q) in 64-bit, then fold limbs top->bottom.
    uint64_t q  = Q;
    uint64_t B  = (uint64_t(1u) << 32) % q;  // B = 2^32 mod Q

    uint64_t r = X.x[2] % Q;     // start with top limb (mod Q)
    r = (r * B + X.x[1]) % q;
    r = (r * B + X.x[0]) % q;

    return uint(r);
}

// only used for sieving, 1st specifically
bool Mod0(uint96 X, uint Q)
{
	return Mod(X, Q) == 0;
}

// -------- a = (2*a) mod N, with full carry handling (works for any reduced a < N) --------
void ModDouble96(inout uint96 A, uint96 N) {
    uint64_t acc;

    acc = (uint64_t(A.x[0]) << 1);
    A.x[0] = uint(acc);
    uint64_t carry = acc >> 32;

    acc = (uint64_t(A.x[1]) << 1) + carry;
    A.x[1] = uint(acc);
    carry = acc >> 32;

    acc = (uint64_t(A.x[2]) << 1) + carry;
    A.x[2] = uint(acc);
    carry = acc >> 32;

    uint top = uint(carry); // 0 or 1 (the 2^96 limb)

    // since A < N, 2A < 2N, so at most one subtract is needed
    if (top != 0u || Cmp(A, N) >= 0) {
        Sub96Wide(A, top, N); // top should become 0 after this
    }

    // (optional safety)
    if (Cmp(A, N) >= 0) {
        uint z = 0u;
        Sub96Wide(A, z, N);
    }
}


// -------- Montgomery representation of 1: oneBar = R mod Q, where R = 2^96 --------
void MontOne96_Int(uint96 Q, out uint96 oneBar) {
    oneBar.x = uvec3(1u, 0u, 0u);       // start at 2^0 mod Q
    for (int i = 0; i < 96; i++) {
        ModDouble96(oneBar, Q);         // now oneBar = 2^(i+1) mod Q
    }
    // after 96 doublings: oneBar = 2^96 mod Q
}

// --------- Exp base 2: returns true iff 2^P mod Q == 1 (i.e., Q divides M_P) ---------
// Montgomery version
//
bool tfm(uint96 k) {
	uint96 sq, Q, pp;
	uint192 t;

	// q = 2 * p * k + 1
	to96(P, pp);
	Mul192(k, pp, t);
	Q = t.L;  // q is limited to 96-bits
	Lsh(Q);
	Inc(Q);

	uint96 oneBar;
	if (UseDouble != 0) {
#ifndef NO_DOUBLE
		double qinv =  0.9999999999999lf / toF(Q);
		MontOne96(Q, qinv, oneBar);
#else
		MontOne96_Int(Q, oneBar);
#endif		
	} else {
		MontOne96_Int(Q, oneBar);
	}

	uint n0prime = MontN0Prime(Q);

	uint96 A = oneBar; // 1 in Montgomery form

	int top = int(findMSB(P));
	uint64_t one = 1ul << top;
	
	for (int b = top; b >= 0; b--) {
		bool bit = (P & one) != 0ul;
		one >>= 1;
		
		MontSquare96(A, Q, n0prime);
		if (bit) {
			MontDouble96(A, Q);
		}
	}

	// 2^P mod Q == 1  <=>  A == 1 (in Montgomery) <=> A == oneBar
	return Eq96(A, oneBar);
}

void main() {
	//uint I = gl_GlobalInvocationID.x;
	//
	// K is the base starting value for this invocation. 
	//
	uint96 kbase;
	kbase.x = uvec3(uint(K[0]& 0xffffffff), uint((K[0]>>32)& 0xffffffff), uint(K[1]&0xffffffff));

	// copy some counters back to shared memory
	if (Init == 5) {
		uint I = gl_GlobalInvocationID.x;
		if (I == 0) {
			Debug[0] = xLl;
			Debug[1] = xL2;
		}
		return;
	}
	// initialize atomic counters for the next invocation.
	if (Init >= 10) {
		if (gl_GlobalInvocationID.x == 0) {
			Debug[0] = 0;
			Debug[1] = 0;
			Debug[2] = 0;
			Debug[3] = 0;
			xL = 0;
			L3 = 0;
			NFound = 0;
			if (Init == 12) {
				xL2 = 0;
			}
			if (Init == 11) {
				xLl = 0;
			}
		}
		return;
	}

	// TF run
	if (Init == 0) {
		if (Big > 0) {
			// this version can't handle >96bit
			Debug[3] = 1;
			return;
		}
		while (true) {
			// get the next index from the List to test
			uint i = atomicAdd(xL, 1);
			if (i >= xL2) {
				return;
			}
			uint96 k = kbase;
			uint o = List2[i];

			// XXX: code is faster with this line, even if the condition is never true, weird
			// something magic about the possibility of a continue.
			// doesn't really matter what the condition is.
			if (o == 0 && Zero(k)) {continue;}

			// the actual K this thread will test.
			Add(k, o);

			if (tfm(k)) {
				// How many have we found?
				uint f = atomicAdd(NFound, 1);
				// return the 96-bit K
				Found[f][0] = k.x[0] | uint64_t(k.x[1]) << 32;
				Found[f][1] = k.x[2];
			}
		}
	}

 	// perform the second sieve, populating list2
	if (Init == 2) {
		const uint lim = MnLen - 0;
		uint kmod[lim];
		for (int j = 0; j < lim; j++) {
			kmod[j] = Mod(kbase, Mn[j]);
		}

		while (true) {
			uint i = atomicAdd(xL, 1);

			if (i >= xLl) { return; }
			uint o = List[i];

			bool cc = true;
			for (int j = 0; j < lim; j++) {
				uint ix = (o + kmod[j])%Mn[j];
				bool cx = 0 == (Xx[j][ix/32] & (1u << (ix%32)));
				//if (!cx) {cc = false; }
				cc = cc && cx;
			}
			if (cc)
			{
			     	uint ii = atomicAdd(xL2, 1);
				List2[ii] = o;
			} else if (false) {
				// Give a sample of composites back to the CPU for primality testing.
				// This is just for looking for bugs in the sieving process.
			     	uint ii = atomicAdd(L3, 1);
				if (ii < 1000) {
					Test[ii] = o;
				}
			}
		}
		return;
	}
	// initialize our bit arrays to zero
	if (Init == 3) {
		while (true) {
			uint i = atomicAdd(xL, 1);
			if (i >= 1+Mn[6]/32) {
				// All threads return here.
				return;
			}
			for (int j = 0; j < MnLen; j++) {
				Xx[j][i] = 0;
			}
		}
	}
	// perform the first sieve
	if (Init == 1) {
		uint once = 0;
		while (true) {
			uint i = atomicAdd(xL, 1);
			if (i >= M) {
				// All threads return here.
				return;
			}
			if (once == 0) {
				once = 1;
				// for debugging, count how many threads got scheduled.
				// the atomic op is a little bit expensive, but only the first time in the while loop.
				atomicAdd(Debug[2], 1);
			}
			uint96 K, Q, pp;
			uint192 t;
			to96(P, pp);
			K.x = uvec3(i, 0, 0);
			Mul192(K, pp, t);
			Q = t.L;  // q is limited to 96-bits
			Lsh(Q);
			Inc(Q);

			uint qa7 = Q.x[0] & 7;
			bool y = (qa7 == 3) || (qa7 == 5) || Mod0(Q,3) ||
				Mod0(Q,5) || Mod0(Q,7) || Mod0(Q,11) || Mod0(Q,13) ||
				Mod0(Q,17) || Mod0(Q,19) || Mod0(Q,23);
			if (!y) {
				uint o = atomicAdd(xLl, 1);
				List[o] = i;
			}
			if (i < Mn[2]) {
				bool c = (Mod0(Q,29) || Mod0(Q,31) || Mod0(Q,37)|| Mod0(Q,41) || Mod0(Q,43));
				if (c) {
					atomicOr(Xx[2][i/32], 1u << (i%32));
				}
			}
			if (i < Mn[3]) {
				bool c = (Mod0(Q,47) || Mod0(Q,53) || Mod0(Q,59)|| Mod0(Q,61));
				if (c) {
					atomicOr(Xx[3][i/32], 1u << (i%32));
				}
			}
		       	if (i < Mn[4]) {			     
				bool c = (Mod0(Q,67) || Mod0(Q,71) || Mod0(Q,73)|| Mod0(Q,79));
				if (c) {
					atomicOr(Xx[4][i/32], 1u << (i%32));
				}
			}
			if (i < Mn[5]) {			     
				bool c = (Mod0(Q,83) || Mod0(Q,89) || Mod0(Q,97)|| Mod0(Q,101));
				if (c) {
					atomicOr(Xx[5][i/32], 1u << (i%32));
				}
			}
			if (i < Mn[6]) {			     
				bool c = (Mod0(Q,103)||Mod0(Q,107) || Mod0(Q,109)||Mod0(Q,113));
				if (c) {
					atomicOr(Xx[6][i/32], 1u << (i%32));
				}
			}
			if (i < Mn[7]) {
				bool c = (Mod0(Q,127)||Mod0(Q,131) || Mod0(Q,137));
				if (c) {
					atomicOr(Xx[7][i/32], 1u << (i%32));
				}
			}
			if (i < Mn[8]) {
				bool c = (Mod0(Q,149)||Mod0(Q,151) || Mod0(Q,157));
				if (c) {
					atomicOr(Xx[8][i/32], 1u << (i%32));
				}
			}
			if (i < Mn[9]) {
				bool c = (Mod0(Q,163)||Mod0(Q,167) || Mod0(Q,173));
				if (c) {
					atomicOr(Xx[9][i/32], 1u << (i%32));
				}
			}
			if (i < Mn[0]) {
				bool c = (Mod0(Q,179)||Mod0(Q,181) || Mod0(Q,191));
				if (c) {
					atomicOr(Xx[0][i/32], 1u << (i%32));
				}
			}
			if (i < Mn[1]) {
				bool c = (Mod0(Q,193)||Mod0(Q,197) || Mod0(Q,199));
				if (c) {
					atomicOr(Xx[1][i/32], 1u << (i%32));
				}
			}
			if (i < Mn[10]) {
				bool c = (Mod0(Q,211)||Mod0(Q,223) || Mod0(Q,227));
				if (c) {
					atomicOr(Xx[10][i/32], 1u << (i%32));
				}
			}
			if (i < Mn[11]) {
				bool c = (Mod0(Q,229)||Mod0(Q,233) || Mod0(Q,239));
				if (c) {
					atomicOr(Xx[11][i/32], 1u << (i%32));
				}
			}
			if (i < Mn[12]) {
				bool c = (Mod0(Q,241)||Mod0(Q,251) || Mod0(Q,257));
				if (c) {
					atomicOr(Xx[12][i/32], 1u << (i%32));
				}
			}
			if (i < Mn[13]) {
				bool c = (Mod0(Q,139)||Mod0(Q,263) || Mod0(Q,269));
				if (c) {
					atomicOr(Xx[13][i/32], 1u << (i%32));
				}
			}
			if (i < Mn[14]) {
				bool c = (Mod0(Q,271)||Mod0(Q,277) || Mod0(Q,281));
				if (c) {
					atomicOr(Xx[14][i/32], 1u << (i%32));
				}
			}
			if (i < Mn[15]) {
				bool c = (Mod0(Q,283)||Mod0(Q,293) || Mod0(Q,307));
				if (c) {
					atomicOr(Xx[15][i/32], 1u << (i%32));
				}
			}
			if (i < Mn[16]) {
				bool c = (Mod0(Q,311)||Mod0(Q,313) || Mod0(Q,317));
				if (c) {
					atomicOr(Xx[16][i/32], 1u << (i%32));
				}
			}
			if (i < Mn[17]) {
				bool c = (Mod0(Q,331)||Mod0(Q,337) || Mod0(Q,347));
				if (c) {
					atomicOr(Xx[17][i/32], 1u << (i%32));
				}
			}
			if (i < Mn[18]) {
				bool c = (Mod0(Q,349)||Mod0(Q,353) || Mod0(Q,359));
				if (c) {
					atomicOr(Xx[18][i/32], 1u << (i%32));
				}
			}
			if (i < Mn[19]) {
				bool c = (Mod0(Q,367)||Mod0(Q,373) || Mod0(Q,379));
				if (c) {
					atomicOr(Xx[19][i/32], 1u << (i%32));
				}
			}
			if (i < Mn[20]) {
				bool c = (Mod0(Q,383)||Mod0(Q,389) || Mod0(Q,397));
				if (c) {
					atomicOr(Xx[20][i/32], 1u << (i%32));
				}
			}
			if (i < Mn[21]) {
				bool c = (Mod0(Q,401)||Mod0(Q,409) || Mod0(Q,419));
				if (c) {
					atomicOr(Xx[21][i/32], 1u << (i%32));
				}
			}/*
			if (i < Mn[22]) {
				bool c = (Mod0(Q,421)||Mod0(Q,431) || Mod0(Q,433));
				if (c) {
					atomicOr(Xx[22][i/32], 1u << (i%32));
				}
			}
			if (i < Mn[23]) {
				bool c = (Mod0(Q,439)||Mod0(Q,443) || Mod0(Q,449));
				if (c) {
					atomicOr(Xx[23][i/32], 1u << (i%32));
				}
			}
			if (i < Mn[24]) {
				bool c = (Mod0(Q,457)||Mod0(Q,461) || Mod0(Q,463));
				if (c) {
					atomicOr(Xx[24][i/32], 1u << (i%32));
				}
			}
			if (i < Mn[25]) {
				bool c = (Mod0(Q,467)||Mod0(Q,479) || Mod0(Q,487));
				if (c) {
					atomicOr(Xx[25][i/32], 1u << (i%32));
				}
			}*/
		}
		// no threads reach here.
		return;
	}

}
